{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture et modification des fichiers csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "testA = pd.read_csv(r\"representation.testa.csv\"  ,  header=None)\n",
    "testB = pd.read_csv(r\"representation.testb.csv\"  ,  header=None)\n",
    "train = pd.read_csv(r\"representation.train.csv\"  ,  header=None)\n",
    "\n",
    "true_labels_a=pd.read_csv(r\"true_labels.testa.csv\"  ,  header=None)\n",
    "true_labels_b=pd.read_csv(r\"true_labels.testb.csv\"  ,  header=None)\n",
    "true_labels_train=pd.read_csv(r\"true_labels.train.csv\"  ,  header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modif_df_2(df):\n",
    "    dataframe=df\n",
    "    for k in range(len(dataframe)):\n",
    "        if dataframe[0][k]==\"I-PER\":\n",
    "            dataframe[0][k]=0\n",
    "        elif dataframe[0][k]==\"I-ORG\":\n",
    "            dataframe[0][k]=1\n",
    "        elif dataframe[0][k]==\"I-LOC\":\n",
    "            dataframe[0][k]=2\n",
    "        else :\n",
    "            dataframe[0][k]=3\n",
    "    return dataframe\n",
    "# complexité linéaire\n",
    "\n",
    "true_a=modif_df_2(true_labels_a)\n",
    "true_b=modif_df_2(true_labels_b)\n",
    "true_train=modif_df_2(true_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , Y_train = train, list(true_train[0])\n",
    "X_test_a , Y_test_a = testA,list(true_a[0])\n",
    "X_test_b , Y_test_b = testB,list(true_b[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison des stratégies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimateur utilisé :\n",
    "logit=LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "one=OneVsOneClassifier(logit)\n",
    "rest=OneVsRestClassifier(logit)\n",
    "\n",
    "liste_classifieurs=[rest,one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=0, solver='lbfgs',\n",
       "                                                 tol=0.0001, verbose=0,\n",
       "                                                 warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one.fit(X_train , Y_train)\n",
    "rest.fit(X_train , Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices de confusion :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion(matrice):\n",
    "    df=pd.DataFrame(matrice, index = ['classe Personne', 'classe Org', 'classe Lieu','classe rien'],\n",
    "                    columns = ['prédit Personne', 'prédit Org','predit Lieu','predit rien'])\n",
    "    print(df,\"\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_rest_a=rest.predict(X_test_a)\n",
    "Y_pred_rest_b=rest.predict(X_test_b)\n",
    "Y_pred_one_a=one.predict(X_test_a)\n",
    "Y_pred_one_b=one.predict(X_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST A : \n",
      "\n",
      "One vs Rest : \n",
      "\n",
      "                 prédit Personne  prédit Org  predit Lieu  predit rien\n",
      "classe Personne              101           1            3           24\n",
      "classe Org                     0          75            3           15\n",
      "classe Lieu                    1           3           72            7\n",
      "classe rien                    4           7            9         1675 \n",
      "\n",
      "One vs One : \n",
      "\n",
      "                 prédit Personne  prédit Org  predit Lieu  predit rien\n",
      "classe Personne              108           1            2           18\n",
      "classe Org                     1          76            2           14\n",
      "classe Lieu                    1           8           66            8\n",
      "classe rien                    9          13            7         1666 \n",
      "\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "\n",
      "TEST B : \n",
      "\n",
      "One vs Rest : \n",
      "\n",
      "                 prédit Personne  prédit Org  predit Lieu  predit rien\n",
      "classe Personne              112           3            2           13\n",
      "classe Org                     1          88            9           13\n",
      "classe Lieu                    1           6           86           16\n",
      "classe rien                   14           7            6         1623 \n",
      "\n",
      "One vs One : \n",
      "\n",
      "                 prédit Personne  prédit Org  predit Lieu  predit rien\n",
      "classe Personne              111           4            3           12\n",
      "classe Org                     2          85           10           14\n",
      "classe Lieu                    1           7           88           13\n",
      "classe rien                   14           5            5         1626 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prédit Personne</th>\n",
       "      <th>prédit Org</th>\n",
       "      <th>predit Lieu</th>\n",
       "      <th>predit rien</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>classe Personne</th>\n",
       "      <td>111</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classe Org</th>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classe Lieu</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>88</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classe rien</th>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 prédit Personne  prédit Org  predit Lieu  predit rien\n",
       "classe Personne              111           4            3           12\n",
       "classe Org                     2          85           10           14\n",
       "classe Lieu                    1           7           88           13\n",
       "classe rien                   14           5            5         1626"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"TEST A : \\n\")\n",
    "print(\"One vs Rest : \\n\")\n",
    "df=print_confusion(confusion_matrix(Y_test_a,Y_pred_rest_a))\n",
    "#print(df.to_latex(index=True)) \n",
    "print(\"One vs One : \\n\")\n",
    "df=print_confusion(confusion_matrix(Y_test_a,Y_pred_one_a))\n",
    "#print(df.to_latex(index=True)) \n",
    "print('--'*40,'\\n\\n')\n",
    "print(\"TEST B : \\n\")\n",
    "print(\"One vs Rest : \\n\")\n",
    "print_confusion(confusion_matrix(Y_test_b,Y_pred_rest_b))\n",
    "print(\"One vs One : \\n\")\n",
    "print_confusion(confusion_matrix(Y_test_b,Y_pred_one_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser la régression logistique car c'est la méthode ayant les meilleures performances (comme montré en question 1). Il nous faut maintenant voir quelle stratégie est meilleure : One vs One ou One vs Rest ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algo(liste_classifieurs,X_train,Y_train,X_test,Y_test):    \n",
    "    comparaison, temps_train, temps_inf, acc=[],[],[],[]\n",
    "    for clf in liste_classifieurs:\n",
    "        t1=time.time()\n",
    "        clf.fit(X_train , Y_train)\n",
    "        t2=time.time()\n",
    "        Y_pred=clf.predict(X_test)\n",
    "        report=classification_report(Y_test,Y_pred,output_dict=True,target_names=['Personne','Organisation','Lieu','Rien'])\n",
    "        t3=time.time()\n",
    "        acc.append(clf.score(X_test,Y_test))\n",
    "        temps_train.append(t2-t1)\n",
    "        temps_inf.append(t3-t2)\n",
    "        comparaison.append(pd.DataFrame(report)['Personne'])\n",
    "    df_comparaison=pd.DataFrame(comparaison,index=['One vs Rest','One vs One'])\n",
    "    df_comparaison['temps train (s)']=temps_train\n",
    "    df_comparaison['temps inference (s)']=temps_inf\n",
    "    df_comparaison['accuracy']=acc\n",
    "    return df_comparaison [['accuracy','precision','recall','f1-score','temps train (s)','temps inference (s)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats test A :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>temps train (s)</th>\n",
       "      <th>temps inference (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>One vs Rest</th>\n",
       "      <td>0.9615</td>\n",
       "      <td>0.952830</td>\n",
       "      <td>0.782946</td>\n",
       "      <td>0.859574</td>\n",
       "      <td>4.598698</td>\n",
       "      <td>0.034907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>One vs One</th>\n",
       "      <td>0.9580</td>\n",
       "      <td>0.907563</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>4.269580</td>\n",
       "      <td>0.062832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy  precision    recall  f1-score  temps train (s)  \\\n",
       "One vs Rest    0.9615   0.952830  0.782946  0.859574         4.598698   \n",
       "One vs One     0.9580   0.907563  0.837209  0.870968         4.269580   \n",
       "\n",
       "             temps inference (s)  \n",
       "One vs Rest             0.034907  \n",
       "One vs One              0.062832  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=compare_algo(liste_classifieurs,X_train, Y_train, X_test_a, Y_test_a)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.to_latex(index=True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats test B :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>temps train (s)</th>\n",
       "      <th>temps inference (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>One vs Rest</th>\n",
       "      <td>0.9545</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.868217</td>\n",
       "      <td>4.636598</td>\n",
       "      <td>0.025932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>One vs One</th>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>3.935476</td>\n",
       "      <td>0.071811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy  precision    recall  f1-score  temps train (s)  \\\n",
       "One vs Rest    0.9545   0.875000  0.861538  0.868217         4.636598   \n",
       "One vs One     0.9550   0.867188  0.853846  0.860465         3.935476   \n",
       "\n",
       "             temps inference (s)  \n",
       "One vs Rest             0.025932  \n",
       "One vs One              0.071811  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=compare_algo(liste_classifieurs,X_train, Y_train, X_test_b, Y_test_b)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.to_latex(index=True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque donc sur les deux échantillons de test qu'il est plus difficile d'anonimiser que de faire de l'entity recognition. En effet, l'accuracy est plus élevée que la précision. Cela est très probablement dû au fait qu'il n'y a que 4 classes et que la très grande majorité des token ne sont pas difficilles à classer, ils n'ont  pas d'entité (donc classe rien)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus on remarque que la stratégie One vs One est plus rapide que la stratégie One vs Rest (interpretation?) et que les performances de ces deux stratégies sont comparables. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
